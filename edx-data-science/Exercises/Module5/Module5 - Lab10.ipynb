{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAT210x - Programming with Python for DS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module5- Lab10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.utils.validation import check_random_state\n",
    "import scipy.io.wavfile as wavfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good Luck! Heh."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Samples are Observations. Each audio file will is a single sample in our dataset.\n",
    "\n",
    "Find more information about [Audio Samples here](https://en.wikipedia.org/wiki/Sampling_(signal_processing)).\n",
    "\n",
    "Each .wav file is actually just a bunch of numeric samples, \"sampled\" from the analog signal. Sampling is a type of discretization. When we mention 'samples', we mean observations. When we mention 'audio samples', we mean the actually \"features\" of the audio file.\n",
    "\n",
    "The goal of this lab is to use multi-target, linear regression to generate by extrapolation, the missing portion of the test audio file.\n",
    "\n",
    "Each one audio_sample features will be the output of an equation, which is a function of the provided portion of the audio_samples:\n",
    "\n",
    "    missing_samples = f(provided_samples)\n",
    "\n",
    "You can experiment with how much of the audio you want to chop off and have the computer generate using the Provided_Portion parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play with this. This is how much of the audio file will be provided, in percent. The remaining percent of the file will be generated via linear extrapolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Provided_Portion = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### The Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "You have to download the dataset (audio files) from the website: https://github.com/Jakobovski/free-spoken-digit-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by creating a regular Python List called `zero`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through the dataset and load up all 50 of the `0_jackson*.wav` files using the `wavfile.read()` method: https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.io.wavfile.read.html Be careful! `.read()` returns a tuple and you're only interested in the audio data, and not sample_rate at this point. Inside your for loop, simply append the loaded audio data into your Python list `zero`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero = []\n",
    "for n in range(0,50):\n",
    "    (sample_rate, w) = wavfile.read('./free-spoken-digit-dataset-master/recordings/0_jackson_{0}.wav'.format(n))\n",
    "    zero.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4257, 8000)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(zero[2]), sample_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for a second, convert zero into a DataFrame. When you do so, set the `dtype` to `np.int16`, since the input audio files are 16 bits per sample. If you don't know how to do this, read up on the docs here: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html\n",
    "\n",
    "Since these audio clips are unfortunately not length-normalized, we're going to have to just hard chop them to all be the same length. Since Pandas would have inserted NANs at any spot to make zero a  perfectly rectangular [n_observed_samples, n_audio_samples] array, do a `dropna` on the Y axis here. Then, convert one back into an NDArray using `yourarrayname.values`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>4077</th>\n",
       "      <th>4078</th>\n",
       "      <th>4079</th>\n",
       "      <th>4080</th>\n",
       "      <th>4081</th>\n",
       "      <th>4082</th>\n",
       "      <th>4083</th>\n",
       "      <th>4084</th>\n",
       "      <th>4085</th>\n",
       "      <th>4086</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-369</td>\n",
       "      <td>-431</td>\n",
       "      <td>-475</td>\n",
       "      <td>-543</td>\n",
       "      <td>-571</td>\n",
       "      <td>-557</td>\n",
       "      <td>-528</td>\n",
       "      <td>-455</td>\n",
       "      <td>-394</td>\n",
       "      <td>-305</td>\n",
       "      <td>...</td>\n",
       "      <td>2475</td>\n",
       "      <td>2500</td>\n",
       "      <td>2331</td>\n",
       "      <td>1786</td>\n",
       "      <td>1057</td>\n",
       "      <td>358</td>\n",
       "      <td>158</td>\n",
       "      <td>-108</td>\n",
       "      <td>-402</td>\n",
       "      <td>-884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-311</td>\n",
       "      <td>-91</td>\n",
       "      <td>-140</td>\n",
       "      <td>-182</td>\n",
       "      <td>-271</td>\n",
       "      <td>-68</td>\n",
       "      <td>-235</td>\n",
       "      <td>-359</td>\n",
       "      <td>-129</td>\n",
       "      <td>-198</td>\n",
       "      <td>...</td>\n",
       "      <td>-122</td>\n",
       "      <td>-207</td>\n",
       "      <td>-266</td>\n",
       "      <td>-276</td>\n",
       "      <td>-316</td>\n",
       "      <td>-359</td>\n",
       "      <td>-396</td>\n",
       "      <td>-422</td>\n",
       "      <td>-462</td>\n",
       "      <td>-460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-361</td>\n",
       "      <td>-226</td>\n",
       "      <td>-238</td>\n",
       "      <td>-478</td>\n",
       "      <td>-425</td>\n",
       "      <td>-395</td>\n",
       "      <td>-663</td>\n",
       "      <td>-879</td>\n",
       "      <td>-726</td>\n",
       "      <td>-997</td>\n",
       "      <td>...</td>\n",
       "      <td>-141</td>\n",
       "      <td>-83</td>\n",
       "      <td>0</td>\n",
       "      <td>71</td>\n",
       "      <td>165</td>\n",
       "      <td>284</td>\n",
       "      <td>325</td>\n",
       "      <td>378</td>\n",
       "      <td>416</td>\n",
       "      <td>438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>323</td>\n",
       "      <td>338</td>\n",
       "      <td>357</td>\n",
       "      <td>388</td>\n",
       "      <td>381</td>\n",
       "      <td>393</td>\n",
       "      <td>391</td>\n",
       "      <td>378</td>\n",
       "      <td>348</td>\n",
       "      <td>313</td>\n",
       "      <td>...</td>\n",
       "      <td>1323</td>\n",
       "      <td>1321</td>\n",
       "      <td>1211</td>\n",
       "      <td>827</td>\n",
       "      <td>292</td>\n",
       "      <td>-244</td>\n",
       "      <td>-779</td>\n",
       "      <td>-1241</td>\n",
       "      <td>-1369</td>\n",
       "      <td>-1229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-302</td>\n",
       "      <td>-312</td>\n",
       "      <td>-103</td>\n",
       "      <td>-305</td>\n",
       "      <td>-145</td>\n",
       "      <td>-339</td>\n",
       "      <td>12</td>\n",
       "      <td>-433</td>\n",
       "      <td>-141</td>\n",
       "      <td>-189</td>\n",
       "      <td>...</td>\n",
       "      <td>189</td>\n",
       "      <td>211</td>\n",
       "      <td>189</td>\n",
       "      <td>134</td>\n",
       "      <td>150</td>\n",
       "      <td>224</td>\n",
       "      <td>265</td>\n",
       "      <td>313</td>\n",
       "      <td>328</td>\n",
       "      <td>327</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 4087 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0     1     2     3     4     5     6     7     8     9     ...   4077  \\\n",
       "0  -369  -431  -475  -543  -571  -557  -528  -455  -394  -305  ...   2475   \n",
       "1  -311   -91  -140  -182  -271   -68  -235  -359  -129  -198  ...   -122   \n",
       "2  -361  -226  -238  -478  -425  -395  -663  -879  -726  -997  ...   -141   \n",
       "3   323   338   357   388   381   393   391   378   348   313  ...   1323   \n",
       "4  -302  -312  -103  -305  -145  -339    12  -433  -141  -189  ...    189   \n",
       "\n",
       "   4078  4079  4080  4081  4082  4083  4084  4085  4086  \n",
       "0  2500  2331  1786  1057   358   158  -108  -402  -884  \n",
       "1  -207  -266  -276  -316  -359  -396  -422  -462  -460  \n",
       "2   -83     0    71   165   284   325   378   416   438  \n",
       "3  1321  1211   827   292  -244  -779 -1241 -1369 -1229  \n",
       "4   211   189   134   150   224   265   313   328   327  \n",
       "\n",
       "[5 rows x 4087 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(zero, dtype=np.int16)\n",
    "df = df.dropna(axis=1, how='any')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 4087)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero = df.values\n",
    "zero.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to know how (many audio_samples samples) long the data is now.\n",
    "\n",
    "`zero` is currently shaped like `[n_samples, n_audio_samples]`, so get the `n_audio_samples` count and store it in a variable called `n_audio_samples`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 4087\n"
     ]
    }
   ],
   "source": [
    "(n_samples, n_audio_samples) = zero.shape\n",
    "print( n_samples, n_audio_samples )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create your linear regression model here and store it in a variable called `model`. Don't actually train or do anything else with it yet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 50 takes of each clip. You want to pull out just one of them, randomly, and that one will NOT be used in the training of your model. In other words, the one file we'll be testing / scoring on will be an unseen sample, independent to the rest of your training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave this line alone until you've submitted your lab:\n",
    "rng = check_random_state(7)\n",
    "\n",
    "random_idx = rng.randint(zero.shape[0])\n",
    "test  = zero[random_idx]\n",
    "train = np.delete(zero, [random_idx], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4087,), (49, 4087))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape, train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the shape of `train`, and the shape of `test`.\n",
    "\n",
    "`train` will be shaped: `[n_samples, n_audio_samples]`, where `n_audio_samples` are the 'features' of the audio file \n",
    "\n",
    "`test` will be shaped `[n_audio_features]`, since it is a single sample (audio file, e.g. observation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples, n_audio_samples = train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The test data will have two parts, `X_test` and `y_test`.\n",
    "\n",
    "`X_test` is going to be the first portion of the test audio file, which we will be providing the computer as input. \n",
    "\n",
    "`y_test`, the \"label\" if you will, is going to be the remaining portion of the audio file. Like such, the computer will use linear regression to derive the missing portion of the sound file based off of the training data its received!\n",
    "\n",
    "Let's save the original `test` clip, the one you're about to delete half of, to the current directory so that you can compare it to the 'patched' clip once you've generated it. You should have already got the `sample_rate` when you were loading up the .wav files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavfile.write('Original Test Clip.wav', sample_rate, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the TEST data by creating a slice called `X_test`. It should have `Provided_Portion` * `n_audio_samples` audio sample features, taken from your test audio file, currently stored in variable `test`. In other words, grab the FIRST `Provided_Portion` * `n_audio_samples` audio features from `test` and store it in `X_test`. This should be accomplished using indexing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1021,)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_train_features = int(n_audio_samples * Provided_Portion)\n",
    "X_test = test[:n_train_features]\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the first `Provided_Portion` * `n_audio_samples` features were stored in `X_test`, then we need to also grab the _remaining_ audio features and store them in `y_test`. With the remaining features stored in there, we will be able to R^2 \"score\" how well our algorithm did in completing the sound file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3066,)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = test[n_train_features:]\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Duplicate the same process for `X_train`, `y_train`. The only differences being:\n",
    "\n",
    "1. Your will be getting your audio data from `train` instead of from `test`\n",
    "2. Remember the shape of `train` that you printed out earlier? You want to do this slicing but for ALL samples (observations). For each observation, you want to slice the first `Provided_Portion` * `n_audio_samples` audio features into `X_train`, and the remaining go into `y_train`. All of this should be doable using regular indexing in two lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((49, 1021), (49, 3066))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = train[:,:n_train_features]\n",
    "y_train = train[:,n_train_features:]\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SciKit-Learn gets 'angry' if you don't supply your training data in the form of a 2D dataframe shaped like `[n_samples, n_features]`.\n",
    "\n",
    "So if you only have one SAMPLE, such as is our case with `X_test`, and `y_test`, then by calling `.reshape(1, -1)`, you can turn `[n_features]` into `[1, n_features]` in order to appease SciKit-Learn.\n",
    "\n",
    "On the other hand, if you only have one FEATURE, you can alternatively call `.reshape(-1, 1)` on your data to turn `[n_samples]` into `[n_samples, 1]`.\n",
    "\n",
    "Reshape X_test and y_test as [1, n_features]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 1021), (1, 3066))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = X_test.reshape(1,-1)\n",
    "y_test = y_test.reshape(1,-1)\n",
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit your model using your training data and label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit( X_train, y_train )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use your model to predict the `label` of `X_test`. Store the resulting prediction in a variable called `y_test_prediction`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3066)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_prediction = model.predict(X_test)\n",
    "y_test_prediction.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SciKit-Learn will use float64 to generate your predictions so let's take those values back to int16, which is what our .wav files expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_prediction = y_test_prediction.astype(dtype=np.int16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score how well your prediction would do for some good laughs, by passing in your test data and test label `y_test`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extrapolation R^2 Score:  0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Extrapolation R^2 Score: \", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take the first `Provided_Portion` portion of the test clip, the part you fed into your linear regression model. Then, stitch that together with the 'abomination' the predictor model generated for you and then save the completed audio clip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "completed_clip = np.hstack((X_test, y_test_prediction))\n",
    "wavfile.write('Extrapolated Clip.wav', sample_rate, completed_clip[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats on making it to the end of this crazy lab and module =) !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "58px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
