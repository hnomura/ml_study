{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime \n",
    "from scipy.stats import norm \n",
    "from scipy.stats import multivariate_normal as mvn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import get_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes \n",
    "This example uses Gaussian (Normal) distribution as a model of likelihood.  \n",
    "Mean/Var to each pixel (each of multi-dimentional features) is calculated to each target class, and this is used as likelihood.  \n",
    "During prediction, posterior is calculated by using likelihood and prior (based on count of each class in test data) \n",
    "\n",
    "> Tricky part of the code is use of `multivariate_normal.logpdf( X, mean=mean, cov=var )`.  \n",
    "> MultiVariate calculate joint probability distribution of all features.   \n",
    "> Given covariance is 1D vector, which is recognized as diagonal variances (independence).  \n",
    "> This is products of PDF(x), i.e., summation of logPDF(x).   \n",
    "> This is essential part of **Naive-ness**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes(object):\n",
    "    def fit(self, X, Y, smoothing=1e-2):\n",
    "        # create dictionary of gaussians(mu,var) and prior to each target class \n",
    "        self.gaussians = dict() \n",
    "        self.priors = dict() \n",
    "        labels = set(Y)      # unique set of target label \n",
    "        for c in labels:\n",
    "            current_x = X[Y==c]  # test data with 'c' labeled \n",
    "            \n",
    "            # mean and variance (with smoothing constant) to each pixel (feature), over all test data (axis=0)\n",
    "            # Each mean & var are 1D vector (1xD) \n",
    "            self.gaussians[c] = { \n",
    "                'mean': current_x.mean(axis=0), \n",
    "                'var': current_x.var(axis=0) + smoothing, \n",
    "            }\n",
    "            self.priors[c] = float(len(Y[Y==c])) / len(Y)\n",
    "    \n",
    "    def score(self, X, Y): \n",
    "        P = self.predict(X)\n",
    "        return np.mean( P == Y ) \n",
    "    \n",
    "    def predict(self, X): \n",
    "        N, D = X.shape \n",
    "        K = len(self.gaussians) # the number of target labels \n",
    "        P = np.zeros((N,K))     # For each test data (N), compute probability of each classs (K) \n",
    "        for c,g in self.gaussians.items(): \n",
    "            mean, var = g['mean'], g['var']\n",
    "            # Vectorized to all test data (N)\n",
    "            # X={DxN}, mean={Dx1}, cov={Dx1} \n",
    "            # mvn.logpdf => sum of logpdf( each feature out of D )\n",
    "            P[:,c] = mvn.logpdf(X, mean=mean, cov=var) + np.log(self.priors[c])\n",
    "        # return argmax over classes \n",
    "        return np.argmax( P, axis=1 ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in and transforming data...\n"
     ]
    }
   ],
   "source": [
    "X, Y = get_data(10000)\n",
    "Ntrain = len(Y) // 2 \n",
    "Xtrain, Ytrain = X[:Ntrain], Y[:Ntrain]\n",
    "Xtest, Ytest = X[Ntrain:], Y[Ntrain:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time 0:00:00.068000\n",
      "Train Accuracy:  0.8064\n",
      "Time to compute train accuracy 0:00:01.413000 Train size: 5000\n",
      "Test Accuracy:  0.798\n",
      "Time to compute test accuracy 0:00:01.329000 Test size: 5000\n"
     ]
    }
   ],
   "source": [
    "model = NaiveBayes() \n",
    "\n",
    "t0 = datetime.now() \n",
    "model.fit(Xtrain, Ytrain) \n",
    "print(\"Training Time\", (datetime.now() - t0))\n",
    "\n",
    "t0 = datetime.now() \n",
    "train_score = model.score(Xtrain, Ytrain) \n",
    "print(\"Train Accuracy: \", train_score)\n",
    "print(\"Time to compute train accuracy\", (datetime.now()-t0), \"Train size:\", len(Ytrain)) \n",
    "\n",
    "t0 = datetime.now() \n",
    "test_score = model.score(Xtest, Ytest) \n",
    "print(\"Test Accuracy: \", test_score)\n",
    "print(\"Time to compute test accuracy\", (datetime.now()-t0), \"Test size:\", len(Ytest))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
